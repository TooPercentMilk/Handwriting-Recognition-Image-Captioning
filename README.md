# **Student Handwriting Recognition for Image Captioning - Bradley Kerr**
## Strategy
This task, image captioning, is a very well studied and documented overlap between NLP and CV. Because of this, I decided the best method to train a model was to utilize transfer learning for the majority of the task. 
- For extracting image features, I used the VGG16 architecture. Although this is a detabably outdated model, it demonstrates consistent and reliable performance, and there is plenty of documentation and tutorials on it. This model is directly accessible through the keras applications kit.
- For extracting caption features, I uhad to decide between using pretrained embeddings and learned embeddings. I decided to use a learned embedding layer, since I thought that the particular language in this task is going to have lower diversity than that provided by general-purpose pretrained embeddings. I did add implementation for using pretrained GloVe embeddings as well though, should I change the implementation. Word embeddings are a better soution for this task than other representations like bag-of-words, as most NLP models perform better with dense vectors than sparse. I used the built-in keras LSTM model for the text processing, as LSTM's seem to be the best current method of addressing the vanishing gradient and exploding gradient problems. 

## Data Augmentation
- The first obvious challenge to me was the varying image size. Most of the work I've done in the past has been with fixed input dimensions. The three main options I'm aware of (barring using a different model altogether) are cropping, resizing, or padding. I determined that all methods have downsides, and that the best approach was to use the resize_with_pad method. Just cropping wouldn't work because some of the images are very thin rectangles while others are square. Given that much of the image is text-based, and the captions seemingly rely on the dimensions of the shaped sometimes, I figure simply resizing would stretch the aspect ratio's too much. Resizing while padding the extra space seemed like the best choice to me.
- I considered greyscaling the images as well, but the VGG16 model only accepts 3-channel input, so I wouldn't gain much from doing so.
- For the text, I decided to tokenize the raw captions with the built-in keras tokenizer. I cleaned up the text by removing extra white-space, special characters, and digits. These are generally strong text preprocessing standards, but I was particularly concerned about the use of markdown language in the captions. I decided to ignore those characters nonethelss, since I didn't think there were enough instances to learn meaningful embeddings. 

## Hyperparameters
- **Train/Test Split:** I decided to use an 90/10 split. The dataset is not so diverse, so a smaller test set should still be sufficient for proper evaluation. Nonetheless, I don't want to create an even smaller test set than 10%, since I assume the annotations are from humans, and thus there will be some inherent variance.
- **Dropout:** I split the training data into training and validation splits. I then performed a grid search on the dropout value and found 0.4 to be the best dropout value.
- **Learning Rate:** I split the training data into training and validation splits. I then performed a grid search on the dropout value and found 0.001 to be the best dropout value.
- **Batch Size** Given my limited access to a GPU, I had some contraints with the batch size. I still wanted to have a fairly large batch size, as that would increase model performance. I settled on 32, which is small enough that my computer could run it, while not sacrificing too much accuracy.

## Evaluation
For evaluation, I primarily used the BLEU metric for evaluation. My best model achieved a modest score of 0.13077. This is not great performance compared to the baseline model on the flickr8k dataset, which achieves a score of 0.51688. There are a few possible reasons for this meager performance. 
- Firstly, the dataset contains only a single caption per image, compared to several in the flickr8k dataset. This leads to a smaller corpus, and the size of the corpus is an significant component of training a strong language model. 
- I set the max caption length as 30 words for efficiency. The actual longest caption (barring a single outlier) was 180 words. Perhaps working with a larger max_length would lead to higher cohesion with the ground truth captions.
- The dataset contains images with inconsistent resolution and drawings that don't display obvious scene focus. It could be difficult for the model to accurately distinguish which lines are relevant drawings from a student and which are just background lines from graph paper. This issue could be overcome, but perhaps a model originally intended for flickr8k is not appropriate for this dataset in general. 
In conclusion, the model failed to produce strong results due to the aformentioned limitations, but still has some redeeming qualities. For instance, the example caption generation for the image loaded in my notebook file shows how it could correctly identify the student labeling the side length above it, but was unable to correctly classify the objects in the image.